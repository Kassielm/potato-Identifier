#!/usr/bin/env python3
"""
Script para verificar informaÃ§Ãµes dos modelos TFLite
"""

import os
import sys

def check_model_info():
    """Verifica informaÃ§Ãµes dos modelos disponÃ­veis"""
    
    # Importar TensorFlow Lite
    try:
        import tflite_runtime.interpreter as tflite
        print("âœ… TensorFlow Lite Runtime disponÃ­vel")
    except ImportError:
        try:
            import tensorflow as tf
            tflite = tf.lite
            print("âœ… TensorFlow Lite (do TensorFlow completo) disponÃ­vel")
        except ImportError:
            print("âŒ TensorFlow Lite nÃ£o disponÃ­vel")
            return False
    
    models_dir = "/app/data/models"
    models = [
        "best_int8.tflite",
        "best_float32_edgetpu.tflite", 
        "best_float32.tflite"
    ]
    
    print(f"\nğŸ“ Verificando modelos em: {models_dir}")
    
    for model_name in models:
        model_path = os.path.join(models_dir, model_name)
        print(f"\nğŸ” Analisando: {model_name}")
        
        if not os.path.exists(model_path):
            print(f"   âŒ Arquivo nÃ£o encontrado: {model_path}")
            continue
            
        try:
            # Carregar modelo
            interpreter = tflite.Interpreter(model_path=model_path)
            interpreter.allocate_tensors()
            
            # Obter detalhes de entrada
            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()
            
            print(f"   âœ… Modelo carregado com sucesso")
            print(f"   ğŸ“Š Input shape: {input_details[0]['shape']}")
            print(f"   ğŸ”¢ Input dtype: {input_details[0]['dtype']}")
            print(f"   ğŸ“¤ Outputs: {len(output_details)}")
            
            # Verificar se Ã© quantizado
            if input_details[0]['dtype'] == 'uint8':
                print(f"   ğŸ¯ MODELO QUANTIZADO (INT8) - Ideal para NPU!")
            else:
                print(f"   ğŸ’» Modelo float - CPU/GPU")
                
            # Obter tamanho do arquivo
            file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
            print(f"   ğŸ’¾ Tamanho: {file_size:.2f} MB")
            
        except Exception as e:
            print(f"   âŒ Erro ao carregar modelo: {e}")
    
    return True

if __name__ == "__main__":
    if check_model_info():
        print("\nâœ… VerificaÃ§Ã£o concluÃ­da!")
    else:
        print("\nâŒ VerificaÃ§Ã£o falhou!")
        sys.exit(1)
